{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54579200",
   "metadata": {},
   "source": [
    "# Classifying Medical Text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112b86b6",
   "metadata": {},
   "source": [
    "## Preparations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66563c5b",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a53053ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import re\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f2345e",
   "metadata": {},
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91e24c51",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "question",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "answer",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "source",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "focus_area",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "9a51b0b4-c171-4803-b8de-79c3ae74fb84",
       "rows": [
        [
         "0",
         "What is (are) Glaucoma ?",
         "Glaucoma is a group of diseases that can damage the eye's optic nerve and result in vision loss and blindness. While glaucoma can strike anyone, the risk is much greater for people over 60. How Glaucoma Develops  There are several different types of glaucoma. Most of these involve the drainage system within the eye. At the front of the eye there is a small space called the anterior chamber. A clear fluid flows through this chamber and bathes and nourishes the nearby tissues. (Watch the video to learn more about glaucoma. To enlarge the video, click the brackets in the lower right-hand corner. To reduce the video, press the Escape (Esc) button on your keyboard.) In glaucoma, for still unknown reasons, the fluid drains too slowly out of the eye. As the fluid builds up, the pressure inside the eye rises. Unless this pressure is controlled, it may cause damage to the optic nerve and other parts of the eye and result in loss of vision. Open-angle Glaucoma The most common type of glaucoma is called open-angle glaucoma. In the normal eye, the clear fluid leaves the anterior chamber at the open angle where the cornea and iris meet. When fluid reaches the angle, it flows through a spongy meshwork, like a drain, and leaves the eye. Sometimes, when the fluid reaches the angle, it passes too slowly through the meshwork drain, causing the pressure inside the eye to build. If the pressure damages the optic nerve, open-angle glaucoma -- and vision loss -- may result. There is no cure for glaucoma. Vision lost from the disease cannot be restored. However, there are treatments that may save remaining vision. That is why early diagnosis is important.  See this graphic for a quick overview of glaucoma,  including how many people it affects, whos at risk, what to do if you have it, and how to learn more.  See a glossary of glaucoma terms.",
         "NIHSeniorHealth",
         "Glaucoma"
        ],
        [
         "1",
         "What causes Glaucoma ?",
         "Nearly 2.7 million people have glaucoma, a leading cause of blindness in the United States. Although anyone can get glaucoma, some people are at higher risk. They include - African-Americans over age 40  - everyone over age 60, especially Hispanics/Latinos  - people with a family history of glaucoma. African-Americans over age 40 everyone over age 60, especially Hispanics/Latinos people with a family history of glaucoma.  In addition to age, eye pressure is a risk factor. Whether you develop glaucoma depends on the level of pressure your optic nerve can tolerate without being damaged. This level is different for each person. Thats why a comprehensive dilated eye exam is very important. It can help your eye care professional determine what level of eye pressure is normal for you. Another risk factor for optic nerve damage relates to blood pressure. Thus, it is important to also make sure that your blood pressure is at a proper level for your body by working with your medical doctor. (Watch the animated video to learn more about the causes of glaucoma. To enlarge the video, click the brackets in the lower right-hand corner. To reduce the video, press the Escape (Esc) button on your keyboard.)",
         "NIHSeniorHealth",
         "Glaucoma"
        ],
        [
         "2",
         "What are the symptoms of Glaucoma ?",
         "Symptoms of Glaucoma  Glaucoma can develop in one or both eyes. The most common type of glaucoma, open-angle glaucoma, has no symptoms at first. It causes no pain, and vision seems normal. Without treatment, people with glaucoma will slowly lose their peripheral, or side vision. They seem to be looking through a tunnel. Over time, straight-ahead vision may decrease until no vision remains. Tests for Glaucoma Glaucoma is detected through a comprehensive eye exam that includes a visual acuity test, visual field test, dilated eye exam, tonometry, and pachymetry. (Watch the animated video to learn more about testing for glaucoma. To enlarge the video, click the brackets in the lower right-hand corner. To reduce the video, press the Escape (Esc) button on your keyboard.)  A visual acuity test uses an eye chart test to measure how well you see at various distances. A visual field test measures your side or peripheral vision. It helps your eye care professional tell if you have lost side vision, a sign of glaucoma. In a dilated eye exam, drops are placed in your eyes to widen, or dilate, the pupils. Your eye care professional uses a special magnifying lens to examine your retina and optic nerve for signs of damage and other eye problems. After the exam, your close-up vision may remain blurred for several hours. In tonometry, an instrument measures the pressure inside the eye. Numbing drops may be applied to your eye for this test. With pachymetry,  a numbing drop is applied to your eye. Your eye care professional uses an ultrasonic wave instrument to measure the thickness of your cornea.",
         "NIHSeniorHealth",
         "Glaucoma"
        ],
        [
         "3",
         "What are the treatments for Glaucoma ?",
         "Although open-angle glaucoma cannot be cured, it can usually be controlled. While treatments may save remaining vision, they do not improve sight already lost from glaucoma. The most common treatments for glaucoma are medication and surgery. Medications  Medications for glaucoma may be either in the form of eye drops or pills. Some drugs reduce pressure by slowing the flow of fluid into the eye. Others help to improve fluid drainage. (Watch the video to learn more about coping with glaucoma. To enlarge the video, click the brackets in the lower right-hand corner. To reduce the video, press the Escape (Esc) button on your keyboard.) For most people with glaucoma, regular use of medications will control the increased fluid pressure. But, these drugs may stop working over time. Or, they may cause side effects. If a problem occurs, the eye care professional may select other drugs, change the dose, or suggest other ways to deal with the problem.  Read or listen to ways some patients are coping with glaucoma. Surgery Laser surgery is another treatment for glaucoma. During laser surgery, a strong beam of light is focused on the part of the anterior chamber where the fluid leaves the eye. This results in a series of small changes that makes it easier for fluid to exit the eye. Over time, the effect of laser surgery may wear off. Patients who have this form of surgery may need to keep taking glaucoma drugs. Researching Causes and Treatments Through studies in the laboratory and with patients, NEI is seeking better ways to detect, treat, and prevent vision loss in people with glaucoma. For example, researchers have discovered genes that could help explain how glaucoma damages the eye. NEI also is supporting studies to learn more about who is likely to get glaucoma, when to treat people who have increased eye pressure, and which treatment to use first.",
         "NIHSeniorHealth",
         "Glaucoma"
        ],
        [
         "4",
         "What is (are) Glaucoma ?",
         "Glaucoma is a group of diseases that can damage the eye's optic nerve and result in vision loss and blindness. The most common form of the disease is open-angle glaucoma. With early treatment, you can often protect your eyes against serious vision loss. (Watch the video to learn more about glaucoma. To enlarge the video, click the brackets in the lower right-hand corner. To reduce the video, press the Escape (Esc) button on your keyboard.)  See this graphic for a quick overview of glaucoma, including how many people it affects, whos at risk, what to do if you have it, and how to learn more.  See a glossary of glaucoma terms.",
         "NIHSeniorHealth",
         "Glaucoma"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>source</th>\n",
       "      <th>focus_area</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is (are) Glaucoma ?</td>\n",
       "      <td>Glaucoma is a group of diseases that can damag...</td>\n",
       "      <td>NIHSeniorHealth</td>\n",
       "      <td>Glaucoma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>What causes Glaucoma ?</td>\n",
       "      <td>Nearly 2.7 million people have glaucoma, a lea...</td>\n",
       "      <td>NIHSeniorHealth</td>\n",
       "      <td>Glaucoma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>What are the symptoms of Glaucoma ?</td>\n",
       "      <td>Symptoms of Glaucoma  Glaucoma can develop in ...</td>\n",
       "      <td>NIHSeniorHealth</td>\n",
       "      <td>Glaucoma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What are the treatments for Glaucoma ?</td>\n",
       "      <td>Although open-angle glaucoma cannot be cured, ...</td>\n",
       "      <td>NIHSeniorHealth</td>\n",
       "      <td>Glaucoma</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>What is (are) Glaucoma ?</td>\n",
       "      <td>Glaucoma is a group of diseases that can damag...</td>\n",
       "      <td>NIHSeniorHealth</td>\n",
       "      <td>Glaucoma</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 question  \\\n",
       "0                What is (are) Glaucoma ?   \n",
       "1                  What causes Glaucoma ?   \n",
       "2     What are the symptoms of Glaucoma ?   \n",
       "3  What are the treatments for Glaucoma ?   \n",
       "4                What is (are) Glaucoma ?   \n",
       "\n",
       "                                              answer           source  \\\n",
       "0  Glaucoma is a group of diseases that can damag...  NIHSeniorHealth   \n",
       "1  Nearly 2.7 million people have glaucoma, a lea...  NIHSeniorHealth   \n",
       "2  Symptoms of Glaucoma  Glaucoma can develop in ...  NIHSeniorHealth   \n",
       "3  Although open-angle glaucoma cannot be cured, ...  NIHSeniorHealth   \n",
       "4  Glaucoma is a group of diseases that can damag...  NIHSeniorHealth   \n",
       "\n",
       "  focus_area  \n",
       "0   Glaucoma  \n",
       "1   Glaucoma  \n",
       "2   Glaucoma  \n",
       "3   Glaucoma  \n",
       "4   Glaucoma  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medquad_df = pd.read_csv('datasets/medquad.csv')\n",
    "medquad_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3f4be1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 16412 entries, 0 to 16411\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype \n",
      "---  ------      --------------  ----- \n",
      " 0   question    16412 non-null  object\n",
      " 1   answer      16407 non-null  object\n",
      " 2   source      16412 non-null  object\n",
      " 3   focus_area  16398 non-null  object\n",
      "dtypes: object(4)\n",
      "memory usage: 513.0+ KB\n"
     ]
    }
   ],
   "source": [
    "medquad_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aebf5ed0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5126"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medquad_df['focus_area'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "10d554f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "focus_area",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "count",
         "rawType": "int64",
         "type": "integer"
        }
       ],
       "ref": "dde5ce57-c102-49bf-a091-e9c3ddb8c6a5",
       "rows": [
        [
         "Breast Cancer",
         "53"
        ],
        [
         "Prostate Cancer",
         "43"
        ],
        [
         "Stroke",
         "35"
        ],
        [
         "Skin Cancer",
         "34"
        ],
        [
         "Alzheimer's Disease",
         "30"
        ],
        [
         "Colorectal Cancer",
         "29"
        ],
        [
         "Lung Cancer",
         "29"
        ],
        [
         "Causes of Diabetes",
         "28"
        ],
        [
         "Heart Failure",
         "28"
        ],
        [
         "High Blood Cholesterol",
         "28"
        ],
        [
         "Heart Attack",
         "28"
        ],
        [
         "High Blood Pressure",
         "27"
        ],
        [
         "Parkinson's Disease",
         "25"
        ],
        [
         "Leukemia",
         "22"
        ],
        [
         "Shingles",
         "21"
        ],
        [
         "Osteoporosis",
         "21"
        ],
        [
         "Age-related Macular Degeneration",
         "20"
        ],
        [
         "Diabetes",
         "20"
        ],
        [
         "Hemochromatosis",
         "20"
        ],
        [
         "Diabetic Retinopathy",
         "19"
        ],
        [
         "Psoriasis",
         "19"
        ],
        [
         "Gum (Periodontal) Disease",
         "19"
        ],
        [
         "Kidney Disease",
         "17"
        ],
        [
         "Dry Mouth",
         "16"
        ],
        [
         "Balance Problems",
         "16"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 25
       }
      },
      "text/plain": [
       "focus_area\n",
       "Breast Cancer                       53\n",
       "Prostate Cancer                     43\n",
       "Stroke                              35\n",
       "Skin Cancer                         34\n",
       "Alzheimer's Disease                 30\n",
       "Colorectal Cancer                   29\n",
       "Lung Cancer                         29\n",
       "Causes of Diabetes                  28\n",
       "Heart Failure                       28\n",
       "High Blood Cholesterol              28\n",
       "Heart Attack                        28\n",
       "High Blood Pressure                 27\n",
       "Parkinson's Disease                 25\n",
       "Leukemia                            22\n",
       "Shingles                            21\n",
       "Osteoporosis                        21\n",
       "Age-related Macular Degeneration    20\n",
       "Diabetes                            20\n",
       "Hemochromatosis                     20\n",
       "Diabetic Retinopathy                19\n",
       "Psoriasis                           19\n",
       "Gum (Periodontal) Disease           19\n",
       "Kidney Disease                      17\n",
       "Dry Mouth                           16\n",
       "Balance Problems                    16\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "medquad_df['focus_area'].value_counts()[:25]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edcabd15",
   "metadata": {},
   "source": [
    "### Data Cleaning and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "505b6717",
   "metadata": {},
   "source": [
    "#### Decrease Target Classes Amount"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48897215",
   "metadata": {},
   "source": [
    "We group the top 25 most common focus areas into 5 groups to not classify 5126 classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "69b2d7ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique focus groups: 5\n"
     ]
    }
   ],
   "source": [
    "# Group the top 25 most common focus areas into 5 broader categories (focus groups)\n",
    "focus_area_map = {\n",
    "    'Cancers': ['Breast Cancer', 'Prostate Cancer', 'Skin Cancer', \n",
    "                'Colorectal Cancer', 'Lung Cancer', 'Leukemia'],\n",
    "    \n",
    "    'Cardiovascular Diseases': ['Stroke', 'Heart Failure', 'Heart Attack',\n",
    "                                'High Blood Cholesterol', 'High Blood Pressure'],\n",
    "    \n",
    "    'Metabolic & Endocrine Disorders': ['Causes of Diabetes', 'Diabetes', 'Diabetic Retinopathy',\n",
    "                                        'Hemochromatosis', 'Kidney Disease'],\n",
    "    \n",
    "    'Neurological & Cognitive Disorders': ['Alzheimer\\'s Disease', 'Parkinson\\'s Disease', 'Balance Problems'],\n",
    "    \n",
    "    'Other Age-Related & Immune Disorders': ['Shingles', 'Osteoporosis', 'Age-related Macular Degeneration',\n",
    "                                             'Psoriasis', 'Gum (Periodontal) Disease', 'Dry Mouth']\n",
    "}\n",
    "\n",
    "# Create reverse mapping\n",
    "condition_to_focus_area = {\n",
    "    condition: focus_area\n",
    "    for focus_area, conditions in focus_area_map.items()\n",
    "    for condition in conditions\n",
    "}\n",
    "\n",
    "# Create new column `focus_group` containing focus group\n",
    "medquad_df['focus_group'] = medquad_df['focus_area'].map(condition_to_focus_area)\n",
    "\n",
    "# Verify the number of unique focus groups\n",
    "n_focus_groups = medquad_df['focus_group'].nunique()\n",
    "print(\"Number of unique focus groups:\", n_focus_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "299f7c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before dropping: (16412, 5)\n",
      "Shape after dropping: (647, 5)\n"
     ]
    }
   ],
   "source": [
    "# Drops missing values including those without a focus group\n",
    "print(\"Shape before dropping:\", medquad_df.shape)\n",
    "medquad_df = medquad_df.dropna()\n",
    "print(\"Shape after dropping:\", medquad_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "43bc5a1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "focus_group\n",
      "Cancers                                 210\n",
      "Cardiovascular Diseases                 146\n",
      "Other Age-Related & Immune Disorders    116\n",
      "Metabolic & Endocrine Disorders         104\n",
      "Neurological & Cognitive Disorders       71\n",
      "Name: count, dtype: int64\n",
      "focus_group\n",
      "Cancers                                 0.324575\n",
      "Cardiovascular Diseases                 0.225657\n",
      "Other Age-Related & Immune Disorders    0.179289\n",
      "Metabolic & Endocrine Disorders         0.160742\n",
      "Neurological & Cognitive Disorders      0.109737\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Number of different groups\n",
    "print(medquad_df['focus_group'].value_counts(0))\n",
    "\n",
    "# Percentage of different groups\n",
    "print(medquad_df['focus_group'].value_counts(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f8f865a",
   "metadata": {},
   "source": [
    "#### Analysing Training Texts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0a32ddc",
   "metadata": {},
   "source": [
    "We observe statistics for text lengths. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9d7bf5e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "answer_char_length",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "answer_word_count",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "853d3142-be18-4105-b944-fdf709a8fbb2",
       "rows": [
        [
         "count",
         "647.0",
         "647.0"
        ],
        [
         "mean",
         "2084.3956723338483",
         "330.58423493044825"
        ],
        [
         "std",
         "2736.381686614995",
         "418.234941379491"
        ],
        [
         "min",
         "87.0",
         "10.0"
        ],
        [
         "25%",
         "458.5",
         "75.0"
        ],
        [
         "50%",
         "1066.0",
         "171.0"
        ],
        [
         "75%",
         "2598.0",
         "423.5"
        ],
        [
         "max",
         "29046.0",
         "4281.0"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 8
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>answer_char_length</th>\n",
       "      <th>answer_word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>647.000000</td>\n",
       "      <td>647.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2084.395672</td>\n",
       "      <td>330.584235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>2736.381687</td>\n",
       "      <td>418.234941</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>87.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>458.500000</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1066.000000</td>\n",
       "      <td>171.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>2598.000000</td>\n",
       "      <td>423.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>29046.000000</td>\n",
       "      <td>4281.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       answer_char_length  answer_word_count\n",
       "count          647.000000         647.000000\n",
       "mean          2084.395672         330.584235\n",
       "std           2736.381687         418.234941\n",
       "min             87.000000          10.000000\n",
       "25%            458.500000          75.000000\n",
       "50%           1066.000000         171.000000\n",
       "75%           2598.000000         423.500000\n",
       "max          29046.000000        4281.000000"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Character lengths\n",
    "medquad_df['answer_char_length'] = medquad_df['answer'].astype(str).apply(len)\n",
    "# Word lengths\n",
    "medquad_df['answer_word_count'] = medquad_df['answer'].astype(str).apply(lambda x: len(x.split()))\n",
    "\n",
    "# Calculate descriptive statistics\n",
    "answer_length_stats = medquad_df[['answer_char_length', 'answer_word_count']].describe()\n",
    "answer_length_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d89e8e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the distribution of word counts\n",
    "plt.figure(figsize=(16, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.histplot(medquad_df['answer_word_count'], kde=True, color='skyblue')\n",
    "plt.title('Distribution of Answer Word Counts', fontsize=14)\n",
    "plt.xlabel('Number of Words', fontsize=12)\n",
    "plt.ylabel('Frequency', fontsize=12)\n",
    "plt.axvline(x=answer_length_stats['answer_word_count']['mean'], \n",
    "            color='red', linestyle='--', label=f\"Mean: {answer_length_stats['answer_word_count']['mean']:.1f}\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e76e96",
   "metadata": {},
   "source": [
    "Finally, we have to encode target groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "56c3dac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "focus_map = {\"Neurological & Cognitive Disorders\":0,\n",
    "             \"Cancers\":1,\n",
    "             \"Cardiovascular Diseases\":2,\n",
    "             \"Metabolic & Endocrine Disorders\":3,\n",
    "             \"Other Age-Related & Immune Disorders\":4}\n",
    "\n",
    "medquad_df['focus_group'] = medquad_df['focus_group'].replace(focus_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "559f1dab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape before duplicates: (647, 7)\n",
      "Shape after removing duplicates: (624, 7)\n"
     ]
    }
   ],
   "source": [
    "# Returns all rows that are duplicates of a previous row\n",
    "print(\"Shape before duplicates:\", medquad_df.shape)\n",
    "duplicates = medquad_df[medquad_df.duplicated(subset='answer')]\n",
    "medquad_df = medquad_df.drop_duplicates(subset='answer')\n",
    "print(\"Shape after removing duplicates:\", medquad_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861a91da",
   "metadata": {},
   "source": [
    "#### Split to train and test datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bc9ac823",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, test_df = train_test_split(medquad_df,\n",
    "                                     test_size=0.2,\n",
    "                                     random_state=42,\n",
    "                                     stratify=medquad_df['focus_group'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2460153e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set distribution: focus_group\n",
      "1    0.334669\n",
      "2    0.234469\n",
      "4    0.186373\n",
      "3    0.130261\n",
      "0    0.114228\n",
      "Name: proportion, dtype: float64\n",
      "Testing set distribution: focus_group\n",
      "1    0.336\n",
      "2    0.232\n",
      "4    0.184\n",
      "3    0.136\n",
      "0    0.112\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set distribution:\", train_df['focus_group'].value_counts(1))\n",
    "print(\"Testing set distribution:\", test_df['focus_group'].value_counts(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df27ede0",
   "metadata": {},
   "source": [
    "#### Create Training Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c57dfabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = train_df['answer'].tolist()\n",
    "train_labels = train_df['focus_group'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b882dee4",
   "metadata": {},
   "source": [
    "#### Tokenizing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d04ae31",
   "metadata": {},
   "source": [
    "Before tokenization we remove key words in the texts to prevent data leakage or, at least, to prevent aiming model to finding keywords instead of analysing entire text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c9acaff9",
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_keywords = [\n",
    "    'Breast Cancer', 'Prostate Cancer', 'Skin Cancer',\n",
    "    'Colorectal Cancer', 'Lung Cancer', 'Leukemia', 'Stroke', 'Heart Failure', 'Heart Attack',\n",
    "    'High Blood Cholesterol', 'High Blood Pressure', 'Causes of Diabetes', 'Diabetes', 'Diabetic Retinopathy',\n",
    "    'Hemochromatosis', 'Kidney Disease', 'Alzheimer\\'s Disease', 'Parkinson\\'s Disease', 'Balance Problems',\n",
    "    'Shingles', 'Osteoporosis', 'Age-related Macular Degeneration',\n",
    "    'Psoriasis', 'Gum (Periodontal) Disease', 'Dry Mouth'\n",
    "]\n",
    "\n",
    "# Split all multi-word phrases into individual words \n",
    "# Ex. \"High Blood Cholesterol\" ==> \"High\", \"Blood\", \"Cholesterol\"\n",
    "words_to_remove = set()\n",
    "for phrase in remove_keywords:\n",
    "    for word in re.findall(r'\\b\\w+\\b', phrase):\n",
    "        words_to_remove.add(word.lower())  # lowercased for case-insensitive match\n",
    "\n",
    "# Create regex pattern to match any of the words\n",
    "pattern = re.compile(r'\\b(?:' + '|'.join(map(re.escape, words_to_remove)) + r')\\b', flags=re.IGNORECASE)\n",
    "\n",
    "# Remove individual words from each text\n",
    "masked_train_texts = [pattern.sub('', text) for text in train_texts]\n",
    "\n",
    "# Normalize whitespace\n",
    "masked_train_texts = [re.sub(r'\\s+', ' ', text).strip() for text in masked_train_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50afd1f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokenized_text = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return tokenized_text\n",
    "\n",
    "tokenized_training_corpus = [tokenize(focus_area_text) for focus_area_text in masked_train_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afda89dc",
   "metadata": {},
   "source": [
    "#### Create Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d11d60b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_training_corpus = []\n",
    "for text in tokenized_training_corpus:\n",
    "    for token in text:\n",
    "        combined_training_corpus.append(token)\n",
    "\n",
    "word_freqs = Counter(combined_training_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "be3d0cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Common Words:  [('the', 7750), ('to', 3886), ('and', 3786), ('a', 3469), ('in', 2694), ('or', 2666), ('is', 2301), ('are', 1643), ('for', 1537), ('your', 1523)]\n"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 1000\n",
    "most_common_words = word_freqs.most_common(MAX_VOCAB_SIZE)\n",
    "print(\"Top 10 Most Common Words: \", most_common_words[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f03f182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {word: idx + 2 for idx, (word, freq) in enumerate(most_common_words)}\n",
    "vocab['<unk>'] = 0\n",
    "vocab['<pad>'] = 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbd157e9",
   "metadata": {},
   "source": [
    "#### Encoding Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edf91d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text, vocab):\n",
    "    tokenized_text = tokenize(text)\n",
    "    encoded_text = [vocab.get(word, vocab['<unk>']) for word in tokenized_text]\n",
    "    return encoded_text\n",
    "\n",
    "def pad_or_truncate(encoded_text, max_len):\n",
    "    if len(encoded_text) > max_len:\n",
    "        return encoded_text[:max_len]\n",
    "    else:\n",
    "        return encoded_text + [vocab['<pad>']] * (max_len - len(encoded_text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b27085ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 128\n",
    "padded_train_seqs = [pad_or_truncate(encode_text(seq, vocab), max_len=MAX_SEQ_LENGTH) for seq in train_texts]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce70004",
   "metadata": {},
   "source": [
    "#### Convert to Tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b76faef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tensor = torch.tensor(padded_train_seqs)\n",
    "y_train_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "batch_size = 16\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e71d3278",
   "metadata": {},
   "source": [
    "## Model 1 - FeedForward NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e14aff04",
   "metadata": {},
   "source": [
    "### Build the Neural Network Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6a1843c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "\n",
    "class FeedForwardNNWithEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(FeedForwardNNWithEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "\n",
    "        self.fc1 = nn.Linear(embed_size, hidden_size1)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.fc2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.output = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.output(x)\n",
    "        return x\n",
    "    \n",
    "# Initialize loss function + optimizer\n",
    "vocab_size = len(vocab)\n",
    "embed_size = 50 \n",
    "hidden_size1 = 128\n",
    "hidden_size2 = 64\n",
    "output_size = n_focus_groups\n",
    "\n",
    "model = FeedForwardNNWithEmbedding(vocab_size, embed_size, hidden_size1, hidden_size2, output_size)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7aa9e9a",
   "metadata": {},
   "source": [
    "### Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99795884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 100/500], Average CE Loss: 1.0386927351\n",
      "[Epoch 200/500], Average CE Loss: 0.6570144445\n",
      "[Epoch 300/500], Average CE Loss: 0.3309075059\n",
      "[Epoch 400/500], Average CE Loss: 0.1605443320\n",
      "[Epoch 500/500], Average CE Loss: 0.0581878474\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=5):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        if (epoch + 1) % 100 == 0:\n",
    "            print(f\"[Epoch {epoch + 1}/{num_epochs}], Average CE Loss: {avg_loss:.10f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_dataloader, criterion, optimizer, num_epochs=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e51c882",
   "metadata": {},
   "source": [
    "### Testing Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4b0a8fca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the testing texts and labels to lists\n",
    "test_texts = test_df['answer'].tolist()\n",
    "test_labels = test_df['focus_group'].tolist()\n",
    "\n",
    "# Encode and pad/truncate the testing sequences\n",
    "padded_test_seqs = [pad_or_truncate(encode_text(seq, vocab), MAX_SEQ_LENGTH) for seq in test_texts]\n",
    "\n",
    "# Convert testing sequences to tensors\n",
    "X_tensor_test = torch.tensor(padded_test_seqs)\n",
    "y_tensor_test = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Batch the testing set\n",
    "test_dataset = TensorDataset(X_tensor_test, y_tensor_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e26228aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_predictions_and_probabilities(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    all_probs = [] \n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for batch_X, batch_y in test_loader:\n",
    "            outputs = model(batch_X)\n",
    "            probs = F.softmax(outputs, dim=1)  \n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            predicted_labels = torch.argmax(outputs, dim=1)\n",
    "            all_labels.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "    return all_probs, all_labels\n",
    "\n",
    "# Generate predictions\n",
    "nn_pred_probs, nn_pred_labels = get_predictions_and_probabilities(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "340d5f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[11  1  1  0  1]\n",
      " [ 1 38  0  0  3]\n",
      " [ 1  1 22  2  3]\n",
      " [ 0  0  4  9  4]\n",
      " [ 1  0  2  0 20]]\n",
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "  Neurological & Cognitive Disorders       0.79      0.79      0.79        14\n",
      "                             Cancers       0.95      0.90      0.93        42\n",
      "             Cardiovascular Diseases       0.76      0.76      0.76        29\n",
      "     Metabolic & Endocrine Disorders       0.82      0.53      0.64        17\n",
      "Other Age-Related & Immune Disorders       0.65      0.87      0.74        23\n",
      "\n",
      "                            accuracy                           0.80       125\n",
      "                           macro avg       0.79      0.77      0.77       125\n",
      "                        weighted avg       0.81      0.80      0.80       125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "focus_group_names = [\"Neurological & Cognitive Disorders\", \"Cancers\", \"Cardiovascular Diseases\", \n",
    "                     \"Metabolic & Endocrine Disorders\", \"Other Age-Related & Immune Disorders\"]\n",
    "\n",
    "conf_matrix = confusion_matrix(test_labels, nn_pred_labels)\n",
    "report = classification_report(test_labels, nn_pred_labels, target_names=focus_group_names)\n",
    "\n",
    "print(conf_matrix)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1466bfd5",
   "metadata": {},
   "source": [
    "### Key Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4c7d076",
   "metadata": {},
   "source": [
    "The feedforward neural network with embeddings achieved moderate classification accuracy (0.8) across the five medical focus groups. The confusion matrix and classification report indicate that some groups are easier to distinguish than others, likely due to overlapping terminology and class imbalance. Overall, the model demonstrates the feasibility of automated medical text classification, but further improvements—such as balancing the dataset, refining preprocessing, or using more advanced architectures—could enhance performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9334fa",
   "metadata": {},
   "source": [
    "## Model 2 - Fine-tuned BERT Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b438d07",
   "metadata": {},
   "source": [
    "### Load the pre-trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dcea2bad",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9354e4ce93e47baa3b934246b0b426a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Egor.Tykmanov\\AppData\\Local\\anaconda3\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Egor.Tykmanov\\.cache\\huggingface\\hub\\models--microsoft--BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "484faa1c6aeb4ee2a0230426505f48b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "244e0aa4ab054968adeb3a8b3b9caf2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "079b0d01e6c04f6592510a7cdaf0c210",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56ae80decab34fd3b2d668407c28a5a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Load BiomedBERT model and tokenizer\n",
    "model_name = \"microsoft/BiomedNLP-BiomedBERT-base-uncased-abstract-fulltext\"\n",
    "bert_tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "bert_model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=n_focus_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8b70c137",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Move model to GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "bert_model = bert_model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46864670",
   "metadata": {},
   "source": [
    "### Freeze and unfreeze layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51a25429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: bert.embeddings.word_embeddings.weight | Shape: torch.Size([30522, 768]) | Requires Grad: True\n",
      "Layer: bert.embeddings.position_embeddings.weight | Shape: torch.Size([512, 768]) | Requires Grad: True\n",
      "Layer: bert.embeddings.token_type_embeddings.weight | Shape: torch.Size([2, 768]) | Requires Grad: True\n",
      "Layer: bert.embeddings.LayerNorm.weight | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.embeddings.LayerNorm.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.0.attention.self.query.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.0.attention.self.query.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.0.attention.self.key.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.0.attention.self.key.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.0.attention.self.value.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.0.attention.self.value.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.0.attention.output.dense.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.0.attention.output.dense.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.0.attention.output.LayerNorm.weight | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.0.attention.output.LayerNorm.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.0.intermediate.dense.weight | Shape: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.0.intermediate.dense.bias | Shape: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.0.output.dense.weight | Shape: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.0.output.dense.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.0.output.LayerNorm.weight | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.0.output.LayerNorm.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.1.attention.self.query.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.1.attention.self.query.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.1.attention.self.key.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.1.attention.self.key.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.1.attention.self.value.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.1.attention.self.value.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.1.attention.output.dense.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.1.attention.output.dense.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.1.attention.output.LayerNorm.weight | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.1.attention.output.LayerNorm.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.1.intermediate.dense.weight | Shape: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.1.intermediate.dense.bias | Shape: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.1.output.dense.weight | Shape: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.1.output.dense.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.1.output.LayerNorm.weight | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.1.output.LayerNorm.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.2.attention.self.query.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.2.attention.self.query.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.2.attention.self.key.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.2.attention.self.key.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.2.attention.self.value.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.2.attention.self.value.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.2.attention.output.dense.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.2.attention.output.dense.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.2.attention.output.LayerNorm.weight | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.2.attention.output.LayerNorm.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.2.intermediate.dense.weight | Shape: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.2.intermediate.dense.bias | Shape: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.2.output.dense.weight | Shape: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.2.output.dense.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.2.output.LayerNorm.weight | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.2.output.LayerNorm.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.3.attention.self.query.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.3.attention.self.query.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.3.attention.self.key.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.3.attention.self.key.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.3.attention.self.value.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.3.attention.self.value.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.3.attention.output.dense.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.3.attention.output.dense.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.3.attention.output.LayerNorm.weight | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.3.attention.output.LayerNorm.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.3.intermediate.dense.weight | Shape: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.3.intermediate.dense.bias | Shape: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.3.output.dense.weight | Shape: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.3.output.dense.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.3.output.LayerNorm.weight | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.3.output.LayerNorm.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.4.attention.self.query.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.4.attention.self.query.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.4.attention.self.key.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.4.attention.self.key.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.4.attention.self.value.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.4.attention.self.value.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.4.attention.output.dense.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.4.attention.output.dense.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.4.attention.output.LayerNorm.weight | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.4.attention.output.LayerNorm.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.4.intermediate.dense.weight | Shape: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.4.intermediate.dense.bias | Shape: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.4.output.dense.weight | Shape: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.4.output.dense.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.4.output.LayerNorm.weight | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.4.output.LayerNorm.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.5.attention.self.query.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.5.attention.self.query.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.5.attention.self.key.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.5.attention.self.key.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.5.attention.self.value.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.5.attention.self.value.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.5.attention.output.dense.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.5.attention.output.dense.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.5.attention.output.LayerNorm.weight | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.5.attention.output.LayerNorm.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.5.intermediate.dense.weight | Shape: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.5.intermediate.dense.bias | Shape: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.5.output.dense.weight | Shape: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.5.output.dense.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.5.output.LayerNorm.weight | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.5.output.LayerNorm.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.6.attention.self.query.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.6.attention.self.query.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.6.attention.self.key.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.6.attention.self.key.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.6.attention.self.value.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.6.attention.self.value.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.6.attention.output.dense.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.6.attention.output.dense.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.6.attention.output.LayerNorm.weight | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.6.attention.output.LayerNorm.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.6.intermediate.dense.weight | Shape: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.6.intermediate.dense.bias | Shape: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.6.output.dense.weight | Shape: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.6.output.dense.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.6.output.LayerNorm.weight | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.6.output.LayerNorm.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.7.attention.self.query.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.7.attention.self.query.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.7.attention.self.key.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.7.attention.self.key.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.7.attention.self.value.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.7.attention.self.value.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.7.attention.output.dense.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.7.attention.output.dense.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.7.attention.output.LayerNorm.weight | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.7.attention.output.LayerNorm.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.7.intermediate.dense.weight | Shape: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.7.intermediate.dense.bias | Shape: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.7.output.dense.weight | Shape: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.7.output.dense.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.7.output.LayerNorm.weight | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.7.output.LayerNorm.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.8.attention.self.query.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.8.attention.self.query.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.8.attention.self.key.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.8.attention.self.key.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.8.attention.self.value.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.8.attention.self.value.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.8.attention.output.dense.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.8.attention.output.dense.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.8.attention.output.LayerNorm.weight | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.8.attention.output.LayerNorm.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.8.intermediate.dense.weight | Shape: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.8.intermediate.dense.bias | Shape: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.8.output.dense.weight | Shape: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.8.output.dense.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.8.output.LayerNorm.weight | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.8.output.LayerNorm.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.9.attention.self.query.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.9.attention.self.query.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.9.attention.self.key.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.9.attention.self.key.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.9.attention.self.value.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.9.attention.self.value.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.9.attention.output.dense.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.9.attention.output.dense.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.9.attention.output.LayerNorm.weight | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.9.attention.output.LayerNorm.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.9.intermediate.dense.weight | Shape: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.9.intermediate.dense.bias | Shape: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.9.output.dense.weight | Shape: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.9.output.dense.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.9.output.LayerNorm.weight | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.9.output.LayerNorm.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.10.attention.self.query.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.10.attention.self.query.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.10.attention.self.key.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.10.attention.self.key.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.10.attention.self.value.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.10.attention.self.value.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.10.attention.output.dense.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.10.attention.output.dense.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.10.attention.output.LayerNorm.weight | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.10.attention.output.LayerNorm.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.10.intermediate.dense.weight | Shape: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.10.intermediate.dense.bias | Shape: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.10.output.dense.weight | Shape: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.10.output.dense.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.10.output.LayerNorm.weight | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.10.output.LayerNorm.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.11.attention.self.query.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.11.attention.self.query.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.11.attention.self.key.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.11.attention.self.key.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.11.attention.self.value.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.11.attention.self.value.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.11.attention.output.dense.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.11.attention.output.dense.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.11.attention.output.LayerNorm.weight | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.11.attention.output.LayerNorm.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.11.intermediate.dense.weight | Shape: torch.Size([3072, 768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.11.intermediate.dense.bias | Shape: torch.Size([3072]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.11.output.dense.weight | Shape: torch.Size([768, 3072]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.11.output.dense.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.11.output.LayerNorm.weight | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.encoder.layer.11.output.LayerNorm.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: bert.pooler.dense.weight | Shape: torch.Size([768, 768]) | Requires Grad: True\n",
      "Layer: bert.pooler.dense.bias | Shape: torch.Size([768]) | Requires Grad: True\n",
      "Layer: classifier.weight | Shape: torch.Size([5, 768]) | Requires Grad: True\n",
      "Layer: classifier.bias | Shape: torch.Size([5]) | Requires Grad: True\n"
     ]
    }
   ],
   "source": [
    "# Inspect architecture\n",
    "for name, param in bert_model.named_parameters():\n",
    "    print(f\"Layer: {name} | Shape: {param.shape} | Requires Grad: {param.requires_grad}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a60a4040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all layers\n",
    "for param in bert_model.bert.parameters():\n",
    "    param.requires_grad = False\n",
    "# Unfreeze the classifier layer\n",
    "for param in bert_model.classifier.parameters():\n",
    "    param.requires_grad = True\n",
    "# Unfreeze the pooler layer\n",
    "for param in bert_model.bert.pooler.parameters():\n",
    "    param.requires_grad = True\n",
    "# Unfreeze the last 3 layers of the encoder (layers 9-11)\n",
    "for i in range(9, 12):\n",
    "    for param in bert_model.bert.encoder.layer[i].parameters():\n",
    "        param.requires_grad = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56e4f11",
   "metadata": {},
   "source": [
    "### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "297381ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 512\n",
    "\n",
    "X_train = bert_tokenizer(masked_train_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=MAX_SEQ_LENGTH)\n",
    "y_train = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "train_dataset = TensorDataset(\n",
    "    X_train['input_ids'].to(device), \n",
    "    X_train['attention_mask'].to(device), \n",
    "    y_train.to(device)\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412fae15",
   "metadata": {},
   "source": [
    "### Fine-Tuning Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "2ec9c166",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, bert_model.parameters()), \n",
    "                              lr=0.00001)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ad2018cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average CE Loss: 1.5691405273973942\n",
      "Epoch 2, Average CE Loss: 1.4481211081147194\n",
      "Epoch 3, Average CE Loss: 1.3312761299312115\n",
      "Epoch 4, Average CE Loss: 1.046698346734047\n",
      "Epoch 5, Average CE Loss: 0.6922806915827096\n",
      "Epoch 6, Average CE Loss: 0.4616279462352395\n",
      "Epoch 7, Average CE Loss: 0.3416350884363055\n",
      "Epoch 8, Average CE Loss: 0.2845331118442118\n",
      "Epoch 9, Average CE Loss: 0.18814481468871236\n",
      "Epoch 10, Average CE Loss: 0.15574884973466396\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    bert_model.train()\n",
    "    total_loss = 0.0\n",
    "    \n",
    "    for batch_X, batch_attention_mask, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = bert_model(input_ids=batch_X, \n",
    "                                     attention_mask=batch_attention_mask)\n",
    "        logits = outputs.logits\n",
    "        loss = criterion(logits, batch_y)\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    if (epoch + 1) % 1 == 0:\n",
    "        print(f'Epoch {epoch+1}, Average CE Loss: {avg_loss}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1594cb3",
   "metadata": {},
   "source": [
    "### Testing Set Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "be120100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize and convert to tensors\n",
    "X_test = bert_tokenizer(test_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=MAX_SEQ_LENGTH)\n",
    "y_test = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "# Batch the testing set\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "test_dataset = TensorDataset(X_test['input_ids'].to(device), \n",
    "                             X_test['attention_mask'].to(device),\n",
    "                             y_test.to(device))\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a9d0ccaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.eval()\n",
    "bert_pred_probs = []\n",
    "bert_pred_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch_X, batch_attention_mask, batch_y in test_loader:\n",
    "        outputs = bert_model(input_ids= batch_X, \n",
    "                                   attention_mask= batch_attention_mask)\n",
    "        logits = outputs.logits\n",
    "        probs = F.softmax(logits, dim=1)\n",
    "        bert_pred_probs.extend(probs.cpu().numpy())\n",
    "        \n",
    "        predicted_labels = torch.argmax(logits, dim=1)\n",
    "        bert_pred_labels.extend(predicted_labels.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e59ff3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[14  0  0  0  0]\n",
      " [ 0 41  0  0  1]\n",
      " [ 0  0 29  0  0]\n",
      " [ 0  0  1 16  0]\n",
      " [ 0  0  0  0 23]]\n",
      "                                      precision    recall  f1-score   support\n",
      "\n",
      "  Neurological & Cognitive Disorders       1.00      1.00      1.00        14\n",
      "                             Cancers       1.00      0.98      0.99        42\n",
      "             Cardiovascular Diseases       0.97      1.00      0.98        29\n",
      "     Metabolic & Endocrine Disorders       1.00      0.94      0.97        17\n",
      "Other Age-Related & Immune Disorders       0.96      1.00      0.98        23\n",
      "\n",
      "                            accuracy                           0.98       125\n",
      "                           macro avg       0.98      0.98      0.98       125\n",
      "                        weighted avg       0.98      0.98      0.98       125\n",
      "\n"
     ]
    }
   ],
   "source": [
    "focus_group_names = [\"Neurological & Cognitive Disorders\", \"Cancers\", \"Cardiovascular Diseases\", \n",
    "                     \"Metabolic & Endocrine Disorders\", \"Other Age-Related & Immune Disorders\"]\n",
    "\n",
    "confusion_matrix = confusion_matrix(test_labels, bert_pred_labels)\n",
    "report = classification_report(test_labels, bert_pred_labels, target_names=focus_group_names)\n",
    "\n",
    "print(confusion_matrix)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49939306",
   "metadata": {},
   "source": [
    "### Key Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfc6da9",
   "metadata": {},
   "source": [
    "The fine-tuned specialized BiomedBERT significantly outperforms the simple neural network, achieving nearly perfect classification scores across all classes in each key metric!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d7471b",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2d5298",
   "metadata": {},
   "source": [
    "In conclusion, in this project, two models were implemented, trained for task classification medical text. The first model is the FeedForward NN With Embeddings, which gave moderate results. The second model is a fine-tuned model based on the pre-trained BERT Transformer. Having a lot complicated architecture and demanding much more computational resources, the second model totally outperformed simple NN and gave almost perfect results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
